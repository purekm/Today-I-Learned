Q1.
여러 글로벌 사이트의 데이터를 단일 Amazon S3 버킷에 최대한 빨리 집계하는 동시에
운영 복잡성을 최소화 하기 위한 가장 적합한 솔루션은?

조건:
각 사이트에서 매일 500GB의 데이터를 수집
각 사이트는 고속 인터넷 연결을 갖고 있음
목표: 최대한 빠르게, 단일 S3 버킷으로 중앙 집중화, 운영 복잡성 최소화

S3 Transfer Acceleration
전 세계 엣지 로케이션을 통해 데이터를 가장 빠른 경로로 S3로 전송
인터넷을 통해 전송하되, 글로벌 가속 제공
특히, 각 사이트에 고속 인터넷이 있다면 효과적임

멀티파트 업로드
큰 파일(500GB)을 빠르게, 병렬로 업로드 가능 → 성능 향상
장애 복구도 쉬움 (부분 재시도 가능)

운영 복잡성 최소화
디바이스 설치나 다른 리전/인스턴스를 거치지 않고, 바로 S3 업로드
관리 포인트 적음 → 가장 간단한 구조

부적절한 구조들:
리전에 먼저 업로드 후 복제
리전마다 버킷 운영해야 함 → 복잡성 증가
데이터 복제는 시간이 걸릴 수 있음 → "최대한 빠르게" 충족 못함

Snowball Edge 사용
Snowball은 오프라인 전송 장치
문제에서는 이미 고속 인터넷이 있다고 명시 → 필요 없음
예약/배송/복원 등 오히려 느림

EC2 + EBS + 스냅샷
매우 복잡한 구성
관리할 인스턴스/EBS/스냅샷 필요 → 복잡도 증가
EBS → S3 복사도 시간이 걸림
##############################################################
Q2. 
독점 앱의 로그 파일을 분석할 수 있는 능력이 필요함
Amazon S3 버킷에 JSON 형식으로 저장
쿼리는 간단하고 주문형으로 실행
솔루션 설계자는 기존 아키텍처에 대한 최소한의 변경으로 분석을 수행
설계자는 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하기 위함

핵심 조건
쿼리는 간단하고, on-demand 실행된다.
기존 아키텍처를 최소한만 변경하고 싶다.

간단한 쿼리를 바로 S3에 있는 데이터에 대해 Athena로 실행하면 됨

부적절한 구조:
모든 컨텐츠를 Redshift로 로드하고, SQL 쿼리를 실행함
-> 쿼리가 간단하면 Athena, 복잡한 쿼리는 Redshift 사용
-> 간단한 쿼리임에도 Redshift에 데이터를 먼저 로드해야 하는 복잡한 방식

On-demand로 실행되므로, CloudWatch Logs에 저장할 필요는 없음
-> CloudWatch Logs는 주기적인 로그 저장 및 분석용으로 적합함, 즉석 쿼리에는 불필요한 설정

AWS Glue를 사용해 로그를 카탈로그화 하고, Spark를 사용해 SQL 쿼리를 실행
-> Glue로 데이터 정리하고, Spark로 처리하는 방식은 과정이 복잡하고, 기존 아키텍처에 변화가 큼

Amazon Redshift
무엇인가요?
→ 데이터 웨어하우스 서비스 (즉, 많은 양의 데이터를 빠르게 분석하는 전용 DB)
언제 쓰나요?
→ 대규모 데이터를 저장하고 정교한 SQL 분석이 필요할 때
→ 예: 회사의 모든 매출 데이터를 정리해서 일별/상품별로 분석
특징
미리 데이터를 Redshift로 로딩해야 함
속도가 빠르지만 비용은 상대적으로 높음

Amazon Athena
무엇인가요?
→ S3에 저장된 데이터를 직접 쿼리할 수 있는 서비스
언제 쓰나요?
→ 간단한 쿼리를 빠르게 실행하고 싶을 때
→ 예: S3에 저장된 로그에서 특정 사용자 검색
특징
S3에서 바로 분석 (데이터 이동 필요 없음)
사용한 쿼리 양만큼만 비용 발생 → 비용 효율적

AWS Glue
무엇인가요?
→ 데이터를 준비하고 정리(ETL) 하는 서비스
→ ETL = Extract(추출), Transform(변환), Load(적재)
언제 쓰나요?
→ 여러 곳에 흩어진 데이터를 분석용으로 정리할 때
→ 예: 여러 CSV 파일을 합치고 날짜 형식을 통일
특징
Glue Crawler: 자동으로 테이블 스키마를 인식해서 카탈로그 생성
보통 Athena나 Redshift 전에 사용


##############################################################
Q3
AWS Organizations를 사용해 여러 부서의 여러 AWS 계정을 관리
S3 버킷에 대한 액세스를 조직 내 계정 사용자로만 제한하려 함
최소한의 운영 오버헤드 

PrincipalOrgID 전역 조건 키를 추가하여 IAM보안 주체만 리소스에 액세스할 수 있도록 함

부적절한 구조:
각 부서에 대한 조직단위 만들고, PrincipalOrgPaths 전역 조건 키를 S3 정책에 추가하여
-> PrincipalOrgPaths는 다중 값 조건 키로, 키 하나에 하나 이상의 값이 포함됨.
Cloud Trail 사용
-> Cloud Trail은 리소스 내역을 기록 및 전송하는 서비스
S3 버킷에 액세스 해야하는 각 사용자에 태그 지정
-> 각 사용자마다 태그를 달아야 하므로 최소 운영 오버헤드에 적합하지 않음

##############################################################
Q4
애플리케이션은 VPC의 EC2 인스턴스에서 실행됨
S3 버킷에 저장된 로그 처리
EC2 인스턴스는 인터넷 연결 없이 S3 버킷에 액세스 해야 함
S3에 대한 프라이빗 네트워크 연결을 제공하는 솔루션은?

A. S3 버킷에 대한 게이트웨이 VPC 엔드포인트를 생성합니다. 

부적절한 구성:
B. EC2에서 S3로 가는게 목적이라 부적절
C. Instance profile은 EC2에게 S3에 접근할 수 있는 권한만 부여함
D. APIGateway는 외부에서 들어오는 요청을 내부로 프록시 처럼 전달함(외부 사용자를 위한 서비스)
Lambda,EC2, DynamoDB등에는 보낼 수 있지만, S3에 직접 연결

VPC는 EC2 인스턴스에서 실행됨
인터넷 연결 없이 S3에 access해야 함
인터넷 연결이 나오면, VPC endpoint와 gateway를 생각해봐라

##############################################################
Q5 
문제 상황
현재 EC2 인스턴스가 각자 다른 가용 영역(AZ)에 존재
각각 자기만의 EBS 볼륨 사용
LoadBalancer가 두 인스턴스에 트래픽을 무작위로 분산시킴
=> 모든 문서를 동시에 볼 수 없음
EBS는 인스턴스 1개에만 연결되는 블록 스토리지라서 EC2-1 EBS에 저장된 문서는 EC2-2에서 볼 수 없음
하지만 ALB는 트래픽을 양쪽 Ec2로 무박위 분산시킴
=> 사용자는 어떤 인스턴스에 연결되느냐에 따라 보이는 문서가 달라짐

두 인스턴스가 같은 문서 데이터에 접근해야 함
=> 공유 스토리지가 필요함

두 EBS 볼륨의 데이터를 EFS로 복사
-> 두 EC2가 같은 파일을 보고, 새 문서도 공유 저장 가능
EBS는 블록 스토리지, EFS는 파일 시스템 
##############################################################
Q6
NFS를 사용하여 온프레미스 네트워크 연결 스토리지에 대용량 파일 저장
비디오 파일의 크기는 1MB~500GB로, 총 70TB 저장 가능
비디오 파일을 S3로 마이그레이션하기로 결정
최소한의 네트워크 대역폭을 사용하면서 가능할 빨리 마이그레이션하기로

NFS: 현재 온프레미스에서 사용하는 파일 시스템
파일 크기: 1MB ~ 500GB → 대용량
전체 크기: 70TB
요구 사항:
빠르게 마이그레이션
네트워크 대역폭 최소화

로컬에서 Snowball 장치에 복사 후, 장치 통째로 AWS에 다시 보냄
AWS가 그 장치에서 S3로 업로드

부적절한 구성:
A. AWS CLI를 사용하여 로컬에서 S3로 복사
=> CLI를 사용해서 S3에 업로드하면 인터넷 대역폭이 엄청 많아짐
C. 파일 게이트웨이 + 퍼블릭 엔드 포인트
=> 온프레미스에서 S3 처럼 보이는 파일 공유를 사용, 대역폭을 그대로 사용하면서 전송해야 함
D. Direct Connect + 파일 게이트 웨이
=> 네트워크 기반이라 대역폭을 많이 씀

##############################################################
Q7
회사에 들어오는 메시지 수집
수집한 메시지를 수십개의 앱과 마이크로 서비스가 소비함
메시지 수는 초당 10만개로 갑자기 증가하기도 함 
이 회사는 솔루션을 분리하고 확장성을 높이려고 함 

SNS는 메시지를 퍼블리시하는 역할
SQS는 메시지를 담는 역할
여러 마이크로 서비스가 각각의 SQS 큐로 메시지를 받아 비동기처리 가능
수평확장성이 뛰어남

부적절한 구성:
A. Amazon Kinesis Data Analytics
=> 스트림 데이터를 실시간 분석하는 용도
B. EC2 + Auto Scaling
=> 마이크로 서비스간 비동기 분리가 안되며, 메시지 큐의 역할이 아님
C. Kinesis Data Streams + Lambda + DynamoDB
=> 단일 샤드는 초당 1MB /1000건 제한이 있어서 병목이 발생하고, DynamoDB에서 읽는 구조는 부적절함
##############################################################
Q8
분산 애플리케이션을 AWS로 마이그레이션
앱은 다양한 워크로드 처리
레거시 플랫폼은 여러 컴퓨팅 노드에서 작업을 조정하는 기본 서버로 구성
탄력성과 확장성을 극대화하는 솔루션

적절한 구성:
SQS + EC2 Auto Scaling(대기열 크기 기반)
=> 작업 양에 따라 자동으로 확장/축소

CPU 사용률에 따라 AutoScaling하려면 Target Tracking Policy를 사용

부적절한 구성:
A. SQS + EC2 Auto Scaling(예약 조정)
=> 미리 정한 시간에만 인스턴스 수 조정하기 때문에 워크로드가 언제 몰릴지 예측 불가하기 때문에 부적절
C. EC2 + CloudTrail
=> CloudTrail은 로깅 서비스 이므로 패스
D. 기본서버 + EventBridge
=> EventBridge는 작업 큐잉 및 조정 역할에는 적합하지 않음
=> CPU 사용량에 따른 auto scaling 이라면 탄력성과 확장성이라고 하긴 어려움
##############################################################
Q9
데이터 센터의 SMB 파일 서버를 실행
파일 서버는 처음 며칠 동안 자주 액세스하는 대용량 파일 저장
7일이 지나면 거의 액세스하지 않음
최근 액세스한 파일에 대한 저지연 액세스를 잃지 않으면서, 저장 공간을 늘려야 함

적절한 구성:
B. S3 파일 게이트웨이는 로컬에 캐시를 유지하면서 수명 주기 정책으로 스토리지 자동 전환 가능

부적절한 구성:
A. AWS DataSync로 7일이 지난 데이터 복사
=> 한번 복사해서 옮기는 건 적합하지만, 실시간 저지연 액세스를 제공하지는 못함
C. Amazon FSx for Windows File Server
=> 확장 가능한 SMB 서버긴 하지만, 자동 수명 관리 기능이 없음
D. 각 사용자 컴퓨터에 S3 접근 유틸리티 설치
=> 사용자가 S3에 접근해야 하므로 관리 어려움
##############################################################
Q10
전자 상 거래 웹 앱구축
API Gateway REST API에 주문 정보 보내고, 접수된 순서대로 처리되길 원함

API Gateway 통합을 사용하여 앱이 주문을 수신할 때, SQS FIFO 대기열에 메시지를 보내고,
처리를 위해 AWS Lambda함수를 호출하도록 SQS FIFO 대기열을 구성
##############################################################
Q11
EC2 인스턴스에서 실행되며, Aurora DB를 사용하는 앱 존재
EC2 인스턴스는 파일에 로컬로 저장된 사용자 정보를 사용하여 DB에 연결
회사는 자격 증명 관리의 운영 오버헤드를 최소화하려고 함

적절한 구성:
A. AWS Secrets Manager 
=> Secrets Manager은 자격 증명을 저장해두고 관리할 수 있는 서비스

부적절한 구성:
B. Systems Manager Parameter Store 
=> DB 문자열과 같은 평문 데이터든 암호와 같은 비밀이든 관계없이 구성 데이터를 관리할 수 있는 중앙 스토어 제공
=> 비밀과 코드를 분리하여 원치않는 노출 막기 위한 시스템
C. KMS
=> KMS 키는 S3 버킷에 저장하는 것이 아니라 Secrets Manager 등을 이용해 관리
D. EBS
=> C와 같은 이유로 오답

##############################################################
Q12
ALB 뒤의 EC2인스턴스에서 웹 앱을 호스팅
웹 앱에는 정적+동적 데이터
정적 데이터를 S3에 저장
정적 데이터 및 동적 데이터의 성능 개선 및 대기시간을 줄이길 원함
Amazon Route53에 등록된 자체 도메인 사용 중

적절한 구성:
A. S3와 ALB를 오리진으로 설정한 CloudFront배포 -> Route 53 연결
=> CloudFront는 전 세계에 있는 엣지 로에키션을 통해 콘텐츠 배포

부적절한 구성:
B. CloudFront는 ALB만 오리진 + Global Accelerator에 S3연결
=> Global Accelerator는 TCP/UDP 기반의 가속기로, 웹 콘텐츠 배포에 적합하지 않음
C. CloudFront는 S3만 오리진 + ALB는 Global Accelerator
=> 콘텐츠 분산이 이원화 되어 관리 복잡도 상승
D. 각각 다른 도메인 사용시 관리 복잡도 상승 및 혼란 발생
##############################################################
Q13
회사는 인프라에 대한 월별 유지 관리
여러 리전에서 Mysql용 rds DB에 대한 자격 증명을 교체해야 함
최소한의 운영 오버헤드로 요구사항을 충족하는 솔루션?

적절한 구성:
A. AWS Secrets Manager 사용 + 다중 리전 복제 + 자동 교체
=> AWS Secrets Managers은 자격 증명을 안전하게 저장/교체/관리
=> 다중 리전 복제 기능도 지원하며, 자동 교체도 스케줄 설정으로 간편하게 가능

부적절한 구성:
B. System Manager Parameter Store
=> 보안 파라미터 저장 가능하지만, 제한적인 부분이 많음
C. S3에 자격 증명을 저장 + Lambda로 교체
=> 보안상 좋지않은 구조
D. KMS + DynamoDB + Lambda
=> 이미 Secrets Manager가 기능을 구현하는데, 이 조합으론 구현해야 함
##############################################################
Q14
인스턴스는 여러 가용 영역에 걸쳐 EC2 Auto Scaling 실행(CPU 사용 기반)
현재 앱 로드가 증가하면 DB 성능이 빠르게 저하되며, 읽기 요청이 쓰기보다 많이 요청됨
회사는 고가용성을 유지하면서, 예측할 수 없는 워크로드의 수요를 충족하도록 자동 Scaling 솔루션을 원함

적절한 구조:
C. 다중 AZ 배포와 함께 Aurora 사용하여 Auto Scaling
=> Aurora는 자동으로 3개의 AZ에 6개의 복제본 생성하므로 고가용성 및 Auto Scaling
=> Aurora는 Mysql보다 5배 향상된 성능을 제공하며, 더 많은 읽기 요청 처리
부적절한 구조:
A. 단일 노드 + Redshift
=> 고가용성 불만족, Redshift는 데이터 웨어하우스
B. 단일 AZ 배포 + rds
=> 단일 AZ는 고 가용성 불만족
D. EC2 스팟 인스턴스 + Memcached용 Elastic Cache
=> 스팟 인스턴스는 언제든지 중지될 위험을 대비해야 하며, 고가용성 불만족
##############################################################
Q15
프로덕션 VPC가 들어오고 나가는 트래픽 보호하는 솔루션 구현하려고 함
자체 데이터 센터에 검사 서버를 가지고 있는데, 이는 트래픽 흐름 검사 및 필터링 같은 특정 작업 수행

적절한 구조:
C. Firewall을 사용하면 VPC 경계에서 네트워크 트래픽 필터링 가능
GuardDuty는 계정 보호 서비스
AWS network 방화벽 서비스가 필터링
AWS firewall Manager은 중앙에서 방화벽 규칙 관리


##############################################################