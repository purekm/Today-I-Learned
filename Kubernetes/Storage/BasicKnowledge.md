```md
# Storage & Filesystem 정리

## 1. SSD 인터페이스 (SATA vs NVMe)

### 1.1 SATA SSD

- **인터페이스:** SATA (Serial ATA)
- **특징**
  - HDD 시절부터 쓰이던 **전통적인 인터페이스**를 그대로 사용
  - 대략 **최대 600MB/s(SATA3 기준)** 정도의 속도 한계
  - 컨트롤러/포트에 여러 장치가 매달리는 구조라, 설계상 병목이 생기기 쉬움
- 장점: 호환성 좋고, 가격이 상대적으로 저렴
- 단점: 인터페이스 한계 때문에 요즘 기준에선 느린 편

### 1.2 NVMe SSD

- **인터페이스:** PCIe + NVMe 프로토콜
- **특징**
  - CPU와 **point-to-point**에 가까운 방식으로 연결 (PCIe 레인 직접 사용)
  - 훨씬 높은 대역폭, 낮은 레이턴시
  - 병렬 I/O 처리에 최적화된 큐 구조를 사용
- 장점: 매우 높은 성능, 특히 랜덤 I/O와 병렬 작업에 강함
- 단점: 가격이 더 비싸고, 구형 시스템에서는 지원이 안 될 수 있음

> 핵심 차이: 디스크의 물리 구조라기보다  
> **“어떤 인터페이스/프로토콜(SATA vs PCIe+NVMe)을 통해 연결되느냐”의 차이**라고 이해하면 된다.

---

## 2. File System 기본 개념

### 2.1 구성 요소

- **Block**
  - 파일 시스템에서 데이터를 저장하는 **기본 단위**
  - 예: 4KB 블록

- **Inode**
  - 파일의 **메타데이터**를 저장
  - 예: 파일 크기, UID/GID, 권한, 생성/수정 시간, 데이터 블록 위치 등
  - 파일 이름은 inode 안에 들어가지 않는다.

- **Directory**
  - “파일 이름 → inode 번호”를 매핑하는 **테이블**
  - 디렉터리는 결국 `(이름, inode 번호)` 리스트라고 보면 됨.

- **Journaling**
  - 파일 시스템의 메타데이터 변경을 **저널에 먼저 기록**한 뒤 실제 데이터에 반영하는 방식
  - 갑작스러운 전원 차단/장애 시:
    - 파일 시스템 **메타데이터의 일관성을 빠르게 복구**할 수 있음
  - 단, 데이터 손실이 “절대 안 생긴다”는 뜻은 아니고,
    - “손상을 최소화/복구 시간 단축”에 가까운 개념

- **Mount**
  - 특정 디스크/파티션/파일 시스템을
  - 리눅스 디렉터리 트리의 한 지점(예: `/mnt/data`)에 **연결하는 과정**

### 2.2 대표적인 파일 시스템

- **Ext4**
  - 대부분의 리눅스 배포판에서 사용하는 기본 파일 시스템
  - 일반적인 범용 용도에 적합

- **XFS**
  - 대용량 파일, 대형 파일 시스템에 강점
  - 병렬 I/O, 대규모 시스템에서 많이 사용

- **NTFS**
  - Windows의 기본 파일 시스템

### 2.3 파일 시스템이 하는 일 요약

1. 디스크를 **블록 단위**로 나눈다.
2. 파일 데이터를 어떤 블록에 저장할지 결정한다.
3. 파일 메타데이터를 **inode에 기록**하고,
4. 디렉터리를 통해 **파일 이름 → inode**를 매핑한다.

---

## 3. Storage 유형

### 3.1 Block Storage

- 디스크를 **블록 단위**로 제공
- OS 입장에서는 **일반 로컬 디스크처럼 보임**
- 장점
  - 빠른 I/O, 낮은 레이턴시
  - DB, VM 디스크 등 **랜덤 I/O 중심 워크로드에 적합**
- 단점
  - 여러 서버가 **같은 블록 디바이스를 동시에 쓰려면**  
    클러스터 파일 시스템(예: GFS2, OCFS2)이 필요
  - 그냥 동시에 마운트하면 파일 시스템이 깨질 수 있으므로,  
    “단순 공유 스토리지” 용도로는 곧바로 쓰기 어려움

### 3.2 File Storage (NAS)

- NFS, SMB 같은 프로토콜로 제공
- 서버/클라이언트 입장에서는 **디렉터리/파일 단위**로 접근
- 장점
  - 여러 서버에서 **동일 공유 폴더** 접근이 쉬움
- 단점
  - Block Storage에 비해 레이턴시와 IOPS가 떨어지는 경우가 많음

### 3.3 Object Storage

- **객체 단위**로 데이터를 저장 (키 + 메타데이터 + 바이트 데이터)
- 예: AWS S3, GCS, Azure Blob
- 장점
  - 거의 무제한에 가까운 **확장성**
  - 상대적으로 저렴, 백업/로그/이미지/영상에 적합
- 단점
  - 일반적인 POSIX 파일 시스템처럼 mount해서 쓰는 구조가 아님  
    (s3fs 같은 도구로 “비슷하게” 마운트할 수는 있지만, 본질은 HTTP API 기반)
  - 메타데이터/일관성 모델 차이 때문에 **랜덤 작은 I/O에는 느리고 부적합**

---

## 4. Sequential I/O vs Random I/O

### 4.1 Sequential I/O

- 디스크에서 **연속된 블록을 순서대로 읽고 쓰는** 방식
- 장점
  - 디스크 헤드 이동(스핀디스크 기준) 또는 내부 관리 오버헤드가 적어 **처리량(throughput)이 높음**
- 예:
  - 대용량 파일 복사
  - 백업/복구 작업
  - 동영상 스트리밍 (대부분 순차 읽기)

### 4.2 Random I/O

- 디스크 여기저기 **임의 위치에 있는 블록을 자주 건드리는** 방식
- 장점
  - 필요한 데이터에 바로 접근 가능 → **응답 시간(레이지)가 중요할 때** 유리
- 예:
  - 데이터베이스(특히 OLTP)
  - 가상 머신 디스크 I/O
  - Key-Value 저장소

> SSD 환경에서는 순차/랜덤의 격차가 HDD보다 훨씬 줄어들지만,  
> 여전히 **랜덤 I/O 많을수록 SSD의 이점이 커진다**고 보면 된다.

---

## 5. RAID

### 5.1 RAID 수준별 요약

- **RAID 0 (Striping)**
  - 데이터만 여러 디스크에 나눠 쓰기
  - 장점: 높은 성능, 용량 합산
  - 단점: **어느 디스크 하나만 죽어도 전체 데이터 손실** (보호 없음)

- **RAID 1 (Mirroring)**
  - 동일한 데이터를 2개 이상의 디스크에 그대로 복제
  - 장점: 한 디스크 장애 시에도 데이터 보존
  - 단점: 용량 효율 50% 수준 (2개 중 1개 용량만 실제 사용)

- **RAID 5 (Striping + Single Parity)**
  - 데이터 + 패리티를 여러 디스크에 분산 저장
  - **1개 디스크 장애 허용**
  - 장애 후 rebuild 시에는 IO 부하, 두 번째 디스크 장애 시 데이터 손실

- **RAID 6 (Striping + Double Parity)**
  - 데이터 + 이중 패리티 분산 저장
  - **2개 디스크 장애 허용**
  - RAID 5보다 더 안전하지만, 쓰기 성능이 더 떨어질 수 있음

- **RAID 10 (1+0)**
  - 먼저 **미러링(RAID 1)** 으로 디스크 쌍을 만들고,
  - 그 미러링 세트들을 **스트라이핑(RAID 0)** 하는 구조
  - 장점:
    - 성능 좋고, 내고장성도 우수
    - 각 미러링 쌍에서 1개씩까지는 동시에 죽어도 살아남을 수 있음
  - 단점:
    - 최소 디스크 4개 이상 필요, 용량 효율 50%

- **RAID 01 (0+1)**
  - 먼저 여러 디스크를 **RAID 0(스트라이핑)** 으로 묶고,
  - 그 스트라이프 세트들을 다시 **RAID 1(미러링)** 으로 묶는 구조
  - 성능은 RAID 10과 비슷하지만, **내고장성이 더 약함**
    - 스트라이프 세트 안의 디스크 하나가 죽으면, 그 스트라이프 전체가 “손상된 세트”가 됨
    - 즉, 하나의 미러링 세트 전체가 사라진 것으로 취급 → 남은 한 세트에 모든 걸 의존
    - 이후 남은 세트에서 디스크 하나 더 죽으면 전체 데이터 손실

> 기억법:  
> - **RAID 10**: “먼저 안전하게 미러링 → 그 다음 빨리 스트라이핑” → 보호 더 강함  
> - **RAID 01**: “먼저 스트라이핑 → 나중에 미러링” → 세트 하나가 죽으면 위험해짐

---

## 6. LVM (Logical Volume Manager)

### 6.1 개념

- 물리 디스크(또는 파티션)를 **추상화해서 “논리 볼륨”으로 관리**하게 해 주는 계층
- 디스크를 자르고 붙이고 늘리는 일을  
  파일 시스템/OS 입장에서 **유연하게** 만들기 위한 기술

### 6.2 구성 요소

- **PV (Physical Volume)**
  - 실제 물리 디스크나 파티션에 LVM 메타데이터를 얹은 것
  - 예: `/dev/sdb`, `/dev/sdc1` 등

- **VG (Volume Group)**
  - 여러 PV를 묶어서 **하나의 큰 풀(pool)** 처럼 보이게 만든 것
  - “디스크 풀”에 가까운 개념

- **LV (Logical Volume)**
  - VG에서 필요한 만큼 잘라 만든 **논리 디스크**
  - OS 입장에서는 `/dev/mapper/vg0-lv_data` 같은 “일반 블록 디바이스”처럼 보임
  - 여기에 파일 시스템(ext4, xfs 등)을 생성해서 사용

### 6.3 장단점

- 장점
  - **유연한 디스크 관리**
    - LV 크기 확장/축소
    - 여러 디스크를 하나의 VG로 묶어서 큰 스토리지처럼 사용
  - **스냅샷 생성**
    - 특정 시점의 LV 상태를 스냅샷으로 만들어 백업/롤백 등에 활용
- 단점
  - 단순한 직결 디스크에 비해 **약간의 성능 오버헤드**가 생길 수 있음
  - 구조가 한 단계 복잡해져서, 잘못 관리하면 복구가 더 어려워질 수 있음

---

## 7. Thick vs Thin Provisioning

### 7.1 Thick Provisioning

- 볼륨/디스크를 만들 때 **지정한 용량을 처음부터 전부 예약**하는 방식
  - 예: 100GB 볼륨 생성 → 실제 스토리지에서도 바로 100GB 차지
- 장점:
  - 예측 가능한 성능, 조각화(fragmentation) 문제 적음
  - “실제 사용량은 적지만 논리 용량을 크게 잡는” 상황에서 과도한 overcommit 방지
- 단점:
  - 실제로 안 쓰는 공간까지 미리 차지 → 스토리지 효율 낮음

### 7.2 Thin Provisioning

- “실제로 쓰는 만큼만 공간 사용”
  - 논리적으로는 100GB 볼륨이라도,
  - 실제로는 10GB만 쓰고 있으면 **10GB만 실제로 차지**
- 장점:
  - 스토리지 **효율**이 좋음
  - 많은 VM/볼륨에 대해 “논리 용량”을 크게 잡아도 물리 스토리지를 절약 가능
- 단점:
  - 실제 사용량이 예상보다 빨리 늘면,
    - 물리 스토리지가 모자라는 상황이 올 수 있음 (over-provisioning 위험)
  - 모니터링/용량 관리가 중요해짐

> 대부분의 하이퍼바이저/클라우드 환경이 **Thin Provisioning을 기본값**으로 많이 사용한다.
